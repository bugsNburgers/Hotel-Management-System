{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# === Hackman (Optimized for CPU, Colab Ready) ===\n",
        "# Upload corpus.txt and test.txt to /content before running\n",
        "# !pip install hmmlearn==0.2.8 tensorflow-cpu==2.15.0 scikit-learn --quiet\n",
        "\n",
        "import os, random, numpy as np, time, math, json, pickle\n",
        "from collections import defaultdict, deque\n",
        "from hmmlearn import hmm\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import tensorflow as tf\n",
        "\n",
        "# ------------- CONFIG -------------\n",
        "CORPUS_FILE = \"./corpus.txt\"\n",
        "TEST_FILE = \"./test.txt\"\n",
        "MAX_WORD_LEN_FOR_HMM = 10\n",
        "HMM_NUM_STATES = 8\n",
        "HMM_NITER = 20\n",
        "MAX_WORD_LEN = 12\n",
        "LIVES = 6\n",
        "NUM_EPISODES = 1500\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.98\n",
        "LR = 1e-3\n",
        "EPS_START, EPS_END, EPS_DECAY = 1.0, 0.05, 0.9995\n",
        "REPLAY_SIZE, MIN_REPLAY_SIZE = 100000, 200\n",
        "TARGET_UPDATE_EVERY = 100\n",
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ------------- BASIC HELPERS -------------\n",
        "ALPHABET = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
        "LETTER_TO_IDX = {c:i for i,c in enumerate(ALPHABET)}\n",
        "IDX_TO_LETTER = {i:c for c,i in LETTER_TO_IDX.items()}\n",
        "\n",
        "def clean_word(w): return ''.join([c for c in w.lower().strip() if c.isalpha()])\n",
        "def load_words(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [clean_word(w) for w in f if clean_word(w)]\n",
        "\n",
        "print(\"Loading data...\")\n",
        "assert os.path.exists(CORPUS_FILE), \"Upload corpus.txt\"\n",
        "assert os.path.exists(TEST_FILE), \"Upload test.txt\"\n",
        "corpus, test_words = load_words(CORPUS_FILE), load_words(TEST_FILE)\n",
        "words_by_len = defaultdict(list)\n",
        "for w in corpus: words_by_len[len(w)].append(w)\n",
        "print(f\"Loaded {len(corpus)} corpus words, {len(test_words)} test words\")\n",
        "\n",
        "# ------------- FAST HMM TRAINING -------------\n",
        "def train_hmms(words_by_len, max_len=MAX_WORD_LEN_FOR_HMM):\n",
        "    models = {}\n",
        "    for L, words in sorted(words_by_len.items()):\n",
        "        if L > max_len or len(words) < 50: continue\n",
        "        X, lengths = [], []\n",
        "        for w in words:\n",
        "            X.extend([LETTER_TO_IDX[c] for c in w])\n",
        "            lengths.append(len(w))\n",
        "        X = np.array(X).reshape(-1,1)\n",
        "        try:\n",
        "            model = hmm.MultinomialHMM(n_components=HMM_NUM_STATES, n_iter=HMM_NITER, verbose=False)\n",
        "            model.n_symbols = 26\n",
        "            model.fit(X, lengths)\n",
        "            models[L] = model\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    print(f\"Trained {len(models)} HMMs\")\n",
        "    return models\n",
        "\n",
        "hmms = train_hmms(words_by_len)\n",
        "\n",
        "def get_hmm_for_length(L):\n",
        "    if L in hmms: return hmms[L]\n",
        "    if not hmms: return None\n",
        "    return hmms[min(hmms.keys(), key=lambda x: abs(x-L))]\n",
        "\n",
        "# Precompute corpus letter frequencies (huge speedup)\n",
        "freq_global = np.zeros(26)\n",
        "for w in corpus:\n",
        "    for c in set(w): freq_global[LETTER_TO_IDX[c]] += 1\n",
        "freq_global /= freq_global.sum()\n",
        "\n",
        "# ------------- PROB VECTOR USING HMM -------------\n",
        "def hmm_letter_prob_vector(masked, guessed):\n",
        "    L = len(masked)\n",
        "    model = get_hmm_for_length(L)\n",
        "    prob = np.ones(26)*1e-9\n",
        "    possible = [w for w in words_by_len.get(L, [])\n",
        "                if all((masked[i] in ['_', w[i]]) for i in range(L))]\n",
        "    if not possible:\n",
        "        prob += freq_global\n",
        "    elif len(possible) < 1000 or model is None:\n",
        "        for w in possible:\n",
        "            for i,ch in enumerate(masked):\n",
        "                if ch == '_': prob[LETTER_TO_IDX[w[i]]] += 1\n",
        "    else:\n",
        "        for w in random.sample(possible, min(1000, len(possible))):\n",
        "            for i,ch in enumerate(masked):\n",
        "                if ch == '_': prob[LETTER_TO_IDX[w[i]]] += 1\n",
        "    for g in guessed: prob[LETTER_TO_IDX[g]] = 0\n",
        "    if prob.sum() == 0: prob = freq_global\n",
        "    return prob / prob.sum()\n",
        "\n",
        "# ------------- ENVIRONMENT -------------\n",
        "class HangmanEnv:\n",
        "    def _init_(self, words, lives=LIVES):\n",
        "        self.words, self.lives = words, lives\n",
        "        self.reset()\n",
        "    def reset(self, word=None):\n",
        "        self.word = clean_word(word or random.choice(self.words))\n",
        "        self.mask = ['_'] * len(self.word)\n",
        "        self.guessed, self.wrong, self.done = set(), 0, False\n",
        "        return self._state()\n",
        "    def _state(self):\n",
        "        prob_vec = hmm_letter_prob_vector(''.join(self.mask), self.guessed)\n",
        "        return {'masked': ''.join(self.mask), 'guessed': set(self.guessed),\n",
        "                'lives_left': self.lives - self.wrong, 'hmm_probs': prob_vec}\n",
        "    def step(self, letter):\n",
        "        if self.done: return self._state(), 0, True, {}\n",
        "        letter = letter.lower()\n",
        "        if letter in self.guessed:\n",
        "            return self._state(), -3, False, {'reason':'repeat'}\n",
        "        self.guessed.add(letter)\n",
        "        if letter in self.word:\n",
        "            for i,c in enumerate(self.word):\n",
        "                if c == letter: self.mask[i] = c\n",
        "            reward = 10\n",
        "            reason = 'correct'\n",
        "        else:\n",
        "            self.wrong += 1\n",
        "            reward, reason = -2, 'wrong'\n",
        "        if '_' not in self.mask:\n",
        "            self.done = True; reward += 30; outcome = 'win'\n",
        "        elif self.wrong >= self.lives:\n",
        "            self.done = True; reward -= 10; outcome = 'lose'\n",
        "        else: outcome = None\n",
        "        info = {'reason': reason, 'outcome': outcome}\n",
        "        return self._state(), reward, self.done, info\n",
        "\n",
        "# ------------- ENCODING -------------\n",
        "def encode_state(s):\n",
        "    masked = s['masked']\n",
        "    pos = np.zeros((MAX_WORD_LEN, 27), np.float32)\n",
        "    for i,ch in enumerate(masked[:MAX_WORD_LEN]):\n",
        "        pos[i, 26 if ch=='_' else LETTER_TO_IDX[ch]] = 1\n",
        "    pos_flat = pos.flatten()\n",
        "    guessed = np.zeros(26, np.float32)\n",
        "    for g in s['guessed']: guessed[LETTER_TO_IDX[g]] = 1\n",
        "    return np.concatenate([pos_flat, guessed, s['hmm_probs'], [s['lives_left']/LIVES]])\n",
        "\n",
        "STATE_DIM = MAX_WORD_LEN*27 + 26 + 26 + 1\n",
        "ACTION_DIM = 26\n",
        "\n",
        "# ------------- DQN AGENT -------------\n",
        "def build_q_net(in_dim, out_dim, lr):\n",
        "    m = models.Sequential([\n",
        "        layers.Input((in_dim,)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(out_dim, activation='linear')\n",
        "    ])\n",
        "    m.compile(optimizer=optimizers.Adam(lr), loss='mse')\n",
        "    return m\n",
        "\n",
        "class DQN:\n",
        "    def _init_(self):\n",
        "        self.model, self.target = build_q_net(STATE_DIM,ACTION_DIM,LR), build_q_net(STATE_DIM,ACTION_DIM,LR)\n",
        "        self.update_target()\n",
        "        self.replay = deque(maxlen=REPLAY_SIZE)\n",
        "        self.eps = EPS_START\n",
        "    def update_target(self): self.target.set_weights(self.model.get_weights())\n",
        "    def act(self, s_vec, mask):\n",
        "        if np.random.rand() < self.eps:\n",
        "            valid = np.nonzero(mask)[0]\n",
        "            return int(np.random.choice(valid)) if len(valid)>0 else np.random.randint(26)\n",
        "        q = self.model.predict(s_vec.reshape(1,-1), verbose=0)[0]\n",
        "        q -= (1e6*(1-mask))\n",
        "        return int(np.argmax(q))\n",
        "    def store(self, *args): self.replay.append(args)\n",
        "    def sample(self, n):\n",
        "        batch = random.sample(self.replay, n)\n",
        "        s,a,r,s2,done,mask = zip(*batch)\n",
        "        return np.array(s),np.array(a),np.array(r),np.array(s2),np.array(done),np.array(mask)\n",
        "    def train(self, bs):\n",
        "        if len(self.replay) < MIN_REPLAY_SIZE: return\n",
        "        s,a,r,s2,done,mask = self.sample(bs)\n",
        "        q_next = self.target.predict(s2, verbose=0)\n",
        "        q_next -= (1e6*(1-mask))\n",
        "        max_q = np.max(q_next,1)\n",
        "        target = self.model.predict(s, verbose=0)\n",
        "        for i in range(bs):\n",
        "            target[i,a[i]] = r[i] if done[i] else r[i]+GAMMA*max_q[i]\n",
        "        self.model.fit(s, target, epochs=1, verbose=0, batch_size=bs)\n",
        "    def decay_eps(self): self.eps = max(EPS_END, self.eps*EPS_DECAY)\n",
        "\n",
        "# ------------- TRAINING -------------\n",
        "env, agent = HangmanEnv(corpus), DQN()\n",
        "print(\"Seeding replay...\")\n",
        "while len(agent.replay) < MIN_REPLAY_SIZE:\n",
        "    s = env.reset(); s_vec = encode_state(s)\n",
        "    done=False\n",
        "    while not done:\n",
        "        mask = np.ones(26,np.float32)\n",
        "        for g in s['guessed']: mask[LETTER_TO_IDX[g]]=0\n",
        "        a = np.random.choice(np.nonzero(mask)[0])\n",
        "        letter = IDX_TO_LETTER[a]\n",
        "        s2,r,done,_ = env.step(letter)\n",
        "        s2v = encode_state(s2)\n",
        "        vm = np.ones(26,np.float32)\n",
        "        for g in s2['guessed']: vm[LETTER_TO_IDX[g]]=0\n",
        "        agent.store(s_vec,a,r,s2v,done,vm)\n",
        "        s,s_vec=s2,s2v\n",
        "print(\"Replay seeded.\")\n",
        "\n",
        "rewards=[]\n",
        "start=time.time()\n",
        "for ep in range(1, NUM_EPISODES+1):\n",
        "    s = env.reset(); s_vec = encode_state(s)\n",
        "    tot, done = 0, False\n",
        "    while not done and len(s['guessed'])<26:\n",
        "        mask = np.ones(26,np.float32)\n",
        "        for g in s['guessed']: mask[LETTER_TO_IDX[g]]=0\n",
        "        a = agent.act(s_vec,mask)\n",
        "        letter=IDX_TO_LETTER[a]\n",
        "        s2,r,done,_ = env.step(letter)\n",
        "        s2v=encode_state(s2)\n",
        "        vm=np.ones(26,np.float32)\n",
        "        for g in s2['guessed']: vm[LETTER_TO_IDX[g]]=0\n",
        "        agent.store(s_vec,a,r,s2v,done,vm)\n",
        "        agent.train(BATCH_SIZE)\n",
        "        tot+=r; s,s_vec=s2,s2v\n",
        "    rewards.append(tot)\n",
        "    agent.decay_eps()\n",
        "    if ep%TARGET_UPDATE_EVERY==0: agent.update_target()\n",
        "    if ep%100==0:\n",
        "        print(f\"Ep{ep}/{NUM_EPISODES} avgR={np.mean(rewards[-100:]):.1f} eps={agent.eps:.2f} time={(time.time()-start)/60:.1f}m\")\n",
        "\n",
        "# ------------- EVALUATION -------------\n",
        "def evaluate(agent, test_words, n=500):\n",
        "    env=HangmanEnv(test_words)\n",
        "    wins=wrong=rep=0\n",
        "    for w in random.sample(test_words, min(n,len(test_words))):\n",
        "        s=env.reset(w); done=False\n",
        "        while not done:\n",
        "            mask=np.ones(26,np.float32)\n",
        "            for g in s['guessed']: mask[LETTER_TO_IDX[g]]=0\n",
        "            if mask.sum()==0: break\n",
        "            s_vec=encode_state(s)\n",
        "            a=agent.act(s_vec,mask)\n",
        "            s,r,done,info=env.step(IDX_TO_LETTER[a])\n",
        "            if info.get('reason')=='wrong': wrong+=1\n",
        "            if info.get('reason')=='repeat': rep+=1\n",
        "        if '_' not in env.mask: wins+=1\n",
        "    games=min(n,len(test_words))\n",
        "    return {'games':games,'wins':wins,'success_rate':wins/games,\n",
        "            'avg_wrong':wrong/games,'avg_repeat':rep/games}\n",
        "\n",
        "print(\"Evaluating...\")\n",
        "res=evaluate(agent,test_words)\n",
        "print(res)\n",
        "\n",
        "# Save\n",
        "os.makedirs(\"hackman_models\", exist_ok=True)\n",
        "agent.model.save(\"hackman_models/dqn_model.h5\")\n",
        "with open(\"hackman_models/hmms.pkl\",\"wb\") as f: pickle.dump(hmms,f)\n",
        "json.dump(res, open(\"hackman_models/eval.json\",\"w\"), indent=2)\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "RFPZPXzbQeJC",
        "outputId": "e7a83a49-ef71-4b59-84f6-ea31407c3e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'hmmlearn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-648076250.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhmmlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hmmlearn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}